import streamlit as st
import ollama
import fitz
import os

# Function to extract text from PDF
def extract_text_from_pdf(uploaded_file):
    doc = fitz.open(uploaded_file)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# Function to chunk large text for context
def chunk_text(text, max_length=1000):
    # Split the text into chunks with a maximum character length
    chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
    return chunks

def save_uploaded_file(uploaded_file):
    # Get the current working directory
    save_path = os.getcwd()
    # Create the full path for the file
    file_path = os.path.join(save_path, uploaded_file.name)
    
    # Save the file
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
        
    return st.success(f"Saved file: {uploaded_file.name} to {save_path}")

st.title("Chat with PDF!!!")

# Slider for description
st.subheader("How to Use This Application:")
st.slider(
    "Slide to explore the usage description", 
    min_value=0, max_value=100, step=1, value=50
)

# Displaying detailed description and instructions
st.markdown("""
    ### Welcome to the 'Chat with PDF' Application!

    **Description**:  
    This web application allows you to interact with the contents of a PDF document by uploading a file and asking questions about it. The application processes the uploaded PDF, extracts the text, and uses a powerful Large Language Model (LLM) to respond to your questions in real time.

    **Model Used**:  
    The application leverages the **Ollama LLM** (specifically `llama3.1` model), which is capable of understanding the text from the PDF and providing answers. The model is fine-tuned to handle natural language processing tasks and is adept at working with both short and long texts.

    **How It Works**:
    1. **Upload a PDF File**:  
       Use the file uploader to select and upload the PDF file you wish to analyze. The file should be in `.pdf` format.
    
    2. **Text Extraction**:  
       The application extracts the text from the uploaded PDF using the `PyMuPDF` library (imported as `fitz`). This library enables the reading and extraction of text from each page in the PDF.
    
    3. **Text Chunking**:  
       The extracted text may be very large, so it is divided into smaller chunks to facilitate better processing. By default, each chunk contains up to 1000 characters. These chunks serve as context for answering questions.
    
    4. **Ask Questions**:  
       After the text is processed and chunked, you can ask questions related to the content of the PDF. Simply type your question in the text area provided on the app.
    
    5. **Model Response**:  
       When you ask a question, the app sends the prompt (your question) along with the relevant chunk of text to the `llama3.1` model. The model then generates a response based on the content it was provided. The response is displayed in the app.
    
    6. **Receive Insights**:  
       The answers are tailored to the content of the PDF, providing detailed, context-specific insights to help you better understand the document.

    **Features**:
    - Upload any PDF document for analysis.
    - Ask natural language questions based on the document's content.
    - Get accurate and context-aware responses generated by a state-of-the-art LLM.
    - Split large documents into manageable chunks for optimal performance.

    **Why Use This App?**
    - If you're reading a long PDF and need quick answers, this tool can assist by summarizing sections of the document or directly answering your specific questions.
    - Useful for academic research, legal documents, technical papers, or any lengthy PDF content that needs to be understood quickly.
    
    **Try it now** and start chatting with your PDF to gain insights faster and more efficiently!
""")

uploaded_file = st.file_uploader("Choose a PDF file", type=["pdf"])

if uploaded_file is not None:
    save_uploaded_file(uploaded_file)
    # Extract text from the uploaded PDF
    pdf_text = extract_text_from_pdf(uploaded_file)
    
    # Chunk the extracted text
    text_chunks = chunk_text(pdf_text)
    
    # Display the first chunk as a summary
    st.subheader("PDF Content Summary:")
    st.write(text_chunks[0])  # Display first chunk
    
    # Input for user prompt
    prompt = st.text_area(label="Ask a question based on the PDF content")
    button = st.button("Ok")
    
    if button:
        if prompt:
            # Select a chunk of text to send with the prompt
            chunk_to_send = text_chunks[0]  # You could select based on user's query
            combined_prompt = f"Based on the following content: {chunk_to_send}\n\nQuestions: {prompt}"
            response = ollama.generate(model="llama3.1", prompt=combined_prompt)
            st.markdown(response["response"])
